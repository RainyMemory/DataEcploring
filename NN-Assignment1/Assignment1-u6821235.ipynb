{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["# The data used in this notebook is retrieved from: Rahman, J.S., Gedeon, T., Caldwell, S. and Jones, R., 2020, July. Brain Melody Informatics: Analysing Effects of Music on Brainwave Patterns. In 2020 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE.\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"/kaggle/input/comp8420-musicaffect/music-affect/music-eeg-features.xlsx\n/kaggle/input/comp8420-musicaffect/music-affect/README.txt\n/kaggle/input/comp8420-musicaffect/music-affect/jessica_eeg_paper.pdf\n/kaggle/input/comp8420-musicaffect/music-affect/citation.txt\n","output_type":"stream"}]},{"cell_type":"code","source":["!pip install openpyxl \n","!pip install xlrd==1.1.0"],"metadata":{"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /opt/conda/lib/python3.7/site-packages (3.0.7)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl) (1.0.1)\nRequirement already satisfied: xlrd==1.1.0 in /opt/conda/lib/python3.7/site-packages (1.1.0)\n","output_type":"stream"}]},{"cell_type":"code","source":["data_source = pd.read_excel('/kaggle/input/comp8420-musicaffect/music-affect/music-eeg-features.xlsx', header=1)\n","data_source.info()"],"metadata":{"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 576 entries, 0 to 575\nData columns (total 27 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   subject no.          576 non-null    int64  \n 1   mean_F7              576 non-null    float64\n 2   max_F7               576 non-null    float64\n 3   min_F7               576 non-null    float64\n 4   std_F7               576 non-null    float64\n 5   iqr_F7               576 non-null    float64\n 6   var_F7               576 non-null    float64\n 7   sum_F7               576 non-null    float64\n 8   skw_F7               576 non-null    float64\n 9   krt_F7               576 non-null    float64\n 10  mean_first_diff_F7   576 non-null    float64\n 11  mean_second_diff_F7  576 non-null    float64\n 12  rms_F7               576 non-null    float64\n 13  abssum_F7            576 non-null    float64\n 14  ssi_F7               576 non-null    float64\n 15  var_F7.1             576 non-null    float64\n 16  mav_F7               576 non-null    float64\n 17  log_F7               576 non-null    float64\n 18  aac_F7               576 non-null    float64\n 19  dasdv_F7             576 non-null    float64\n 20  dfa_F7               576 non-null    float64\n 21  fuzzy_F7             576 non-null    float64\n 22  shannon_F7           576 non-null    float64\n 23  pe_F7                576 non-null    float64\n 24  hjorth_F7            576 non-null    float64\n 25  hurst_F7             576 non-null    float64\n 26  label                576 non-null    int64  \ndtypes: float64(25), int64(2)\nmemory usage: 121.6 KB\n","output_type":"stream"}]},{"cell_type":"code","source":["data_source.describe()"],"metadata":{"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"       subject no.     mean_F7      max_F7      min_F7      std_F7  \\\ncount   576.000000  576.000000  576.000000  576.000000  576.000000   \nmean     12.500000    1.819086    6.354124    0.243775    1.900527   \nstd       6.928203    2.570039    9.783731    0.763617    2.931105   \nmin       1.000000    0.177153    0.331130    0.000036    0.058098   \n25%       6.750000    0.489684    1.227658    0.005698    0.363696   \n50%      12.500000    0.814839    2.014539    0.015316    0.580049   \n75%      18.250000    1.632414    6.132998    0.230895    1.885264   \nmax      24.000000   21.310083   49.653301   14.591949   16.243698   \n\n           iqr_F7      var_F7      sum_F7      skw_F7      krt_F7  ...  \\\ncount  576.000000  576.000000  576.000000  576.000000  576.000000  ...   \nmean     1.902208   12.188467   24.834008    0.857294    3.136624  ...   \nstd      2.934445   33.388805   37.050789    0.772770    1.626333  ...   \nmin      0.022917    0.003375    1.311274   -1.368202    1.142360  ...   \n25%      0.514053    0.132275    6.050474    0.320250    1.840712  ...   \n50%      0.807612    0.336458    9.727701    0.773900    2.539434  ...   \n75%      1.514906    3.554224   23.708503    1.421059    4.073411  ...   \nmax     22.097017  263.857719  230.562023    2.390834    7.301408  ...   \n\n           log_F7      aac_F7    dasdv_F7      dfa_F7      fuzzy_F7  \\\ncount  576.000000  576.000000  576.000000  576.000000    576.000000   \nmean     0.932747    1.083809    1.661459    1.196636    457.739062   \nstd      1.583103    1.645498    2.552779    1.858145   5446.766319   \nmin      0.091639    0.074650    0.097195    0.007543     -0.090866   \n25%      0.213298    0.220676    0.312948    0.195796      0.505415   \n50%      0.411035    0.371518    0.508042    0.414471      0.907387   \n75%      0.939982    0.999221    1.565938    1.224011      1.747119   \nmax     20.472335   10.685444   13.002317   15.121106  65535.000000   \n\n       shannon_F7       pe_F7   hjorth_F7    hurst_F7       label  \ncount  576.000000  576.000000  576.000000  576.000000  576.000000  \nmean     0.938953    0.475938    0.629106    0.691681    2.000000  \nstd      0.253003    0.204220    0.155269    0.286486    0.817206  \nmin      0.233792    0.000000    0.252908    0.135447    1.000000  \n25%      0.688567    0.218104    0.517218    0.490965    1.000000  \n50%      0.950271    0.581174    0.657355    0.603059    2.000000  \n75%      1.103433    0.637285    0.727326    0.887428    3.000000  \nmax      1.370502    0.724527    1.038237    1.570675    3.000000  \n\n[8 rows x 27 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject no.</th>\n      <th>mean_F7</th>\n      <th>max_F7</th>\n      <th>min_F7</th>\n      <th>std_F7</th>\n      <th>iqr_F7</th>\n      <th>var_F7</th>\n      <th>sum_F7</th>\n      <th>skw_F7</th>\n      <th>krt_F7</th>\n      <th>...</th>\n      <th>log_F7</th>\n      <th>aac_F7</th>\n      <th>dasdv_F7</th>\n      <th>dfa_F7</th>\n      <th>fuzzy_F7</th>\n      <th>shannon_F7</th>\n      <th>pe_F7</th>\n      <th>hjorth_F7</th>\n      <th>hurst_F7</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>...</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n      <td>576.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>12.500000</td>\n      <td>1.819086</td>\n      <td>6.354124</td>\n      <td>0.243775</td>\n      <td>1.900527</td>\n      <td>1.902208</td>\n      <td>12.188467</td>\n      <td>24.834008</td>\n      <td>0.857294</td>\n      <td>3.136624</td>\n      <td>...</td>\n      <td>0.932747</td>\n      <td>1.083809</td>\n      <td>1.661459</td>\n      <td>1.196636</td>\n      <td>457.739062</td>\n      <td>0.938953</td>\n      <td>0.475938</td>\n      <td>0.629106</td>\n      <td>0.691681</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.928203</td>\n      <td>2.570039</td>\n      <td>9.783731</td>\n      <td>0.763617</td>\n      <td>2.931105</td>\n      <td>2.934445</td>\n      <td>33.388805</td>\n      <td>37.050789</td>\n      <td>0.772770</td>\n      <td>1.626333</td>\n      <td>...</td>\n      <td>1.583103</td>\n      <td>1.645498</td>\n      <td>2.552779</td>\n      <td>1.858145</td>\n      <td>5446.766319</td>\n      <td>0.253003</td>\n      <td>0.204220</td>\n      <td>0.155269</td>\n      <td>0.286486</td>\n      <td>0.817206</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.177153</td>\n      <td>0.331130</td>\n      <td>0.000036</td>\n      <td>0.058098</td>\n      <td>0.022917</td>\n      <td>0.003375</td>\n      <td>1.311274</td>\n      <td>-1.368202</td>\n      <td>1.142360</td>\n      <td>...</td>\n      <td>0.091639</td>\n      <td>0.074650</td>\n      <td>0.097195</td>\n      <td>0.007543</td>\n      <td>-0.090866</td>\n      <td>0.233792</td>\n      <td>0.000000</td>\n      <td>0.252908</td>\n      <td>0.135447</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.750000</td>\n      <td>0.489684</td>\n      <td>1.227658</td>\n      <td>0.005698</td>\n      <td>0.363696</td>\n      <td>0.514053</td>\n      <td>0.132275</td>\n      <td>6.050474</td>\n      <td>0.320250</td>\n      <td>1.840712</td>\n      <td>...</td>\n      <td>0.213298</td>\n      <td>0.220676</td>\n      <td>0.312948</td>\n      <td>0.195796</td>\n      <td>0.505415</td>\n      <td>0.688567</td>\n      <td>0.218104</td>\n      <td>0.517218</td>\n      <td>0.490965</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>12.500000</td>\n      <td>0.814839</td>\n      <td>2.014539</td>\n      <td>0.015316</td>\n      <td>0.580049</td>\n      <td>0.807612</td>\n      <td>0.336458</td>\n      <td>9.727701</td>\n      <td>0.773900</td>\n      <td>2.539434</td>\n      <td>...</td>\n      <td>0.411035</td>\n      <td>0.371518</td>\n      <td>0.508042</td>\n      <td>0.414471</td>\n      <td>0.907387</td>\n      <td>0.950271</td>\n      <td>0.581174</td>\n      <td>0.657355</td>\n      <td>0.603059</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>18.250000</td>\n      <td>1.632414</td>\n      <td>6.132998</td>\n      <td>0.230895</td>\n      <td>1.885264</td>\n      <td>1.514906</td>\n      <td>3.554224</td>\n      <td>23.708503</td>\n      <td>1.421059</td>\n      <td>4.073411</td>\n      <td>...</td>\n      <td>0.939982</td>\n      <td>0.999221</td>\n      <td>1.565938</td>\n      <td>1.224011</td>\n      <td>1.747119</td>\n      <td>1.103433</td>\n      <td>0.637285</td>\n      <td>0.727326</td>\n      <td>0.887428</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>24.000000</td>\n      <td>21.310083</td>\n      <td>49.653301</td>\n      <td>14.591949</td>\n      <td>16.243698</td>\n      <td>22.097017</td>\n      <td>263.857719</td>\n      <td>230.562023</td>\n      <td>2.390834</td>\n      <td>7.301408</td>\n      <td>...</td>\n      <td>20.472335</td>\n      <td>10.685444</td>\n      <td>13.002317</td>\n      <td>15.121106</td>\n      <td>65535.000000</td>\n      <td>1.370502</td>\n      <td>0.724527</td>\n      <td>1.038237</td>\n      <td>1.570675</td>\n      <td>3.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 27 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt \n","\n","feature_list = data_source.columns.tolist()\n","feature_list.remove('label') \n","feature_list.remove('subject no.')\n","# for col in feature_list:\n","#     plt.boxplot(x=data_source[col], vert=False, labels=[col])\n","#     plt.show()\n","for col in feature_list:\n","    IQR = data_source[col].quantile(.75) - data_source[col].quantile(.25)\n","    upper_outliers_num = data_source[col][data_source[col] > (data_source[col].quantile(.75) + 1.5 * IQR)].count()\n","    lower_outliers_num = data_source[col][data_source[col] < (data_source[col].quantile(.25) - 1.5 * IQR)].count()\n","    print(\"Number of outliers in\", col, \"is: \", upper_outliers_num + lower_outliers_num)"],"metadata":{"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Number of outliers in mean_F7 is:  88\nNumber of outliers in max_F7 is:  83\nNumber of outliers in min_F7 is:  67\nNumber of outliers in std_F7 is:  85\nNumber of outliers in iqr_F7 is:  86\nNumber of outliers in var_F7 is:  102\nNumber of outliers in sum_F7 is:  77\nNumber of outliers in skw_F7 is:  1\nNumber of outliers in krt_F7 is:  0\nNumber of outliers in mean_first_diff_F7 is:  87\nNumber of outliers in mean_second_diff_F7 is:  88\nNumber of outliers in rms_F7 is:  88\nNumber of outliers in abssum_F7 is:  77\nNumber of outliers in ssi_F7 is:  106\nNumber of outliers in var_F7.1 is:  99\nNumber of outliers in mav_F7 is:  88\nNumber of outliers in log_F7 is:  59\nNumber of outliers in aac_F7 is:  87\nNumber of outliers in dasdv_F7 is:  88\nNumber of outliers in dfa_F7 is:  84\nNumber of outliers in fuzzy_F7 is:  95\nNumber of outliers in shannon_F7 is:  0\nNumber of outliers in pe_F7 is:  0\nNumber of outliers in hjorth_F7 is:  0\nNumber of outliers in hurst_F7 is:  1\n","output_type":"stream"}]},{"cell_type":"code","source":["data_source.std()"],"metadata":{"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"subject no.               6.928203\nmean_F7                   2.570039\nmax_F7                    9.783731\nmin_F7                    0.763617\nstd_F7                    2.931105\niqr_F7                    2.934445\nvar_F7                   33.388805\nsum_F7                   37.050789\nskw_F7                    0.772770\nkrt_F7                    1.626333\nmean_first_diff_F7        1.618207\nmean_second_diff_F7       2.356572\nrms_F7                    3.768027\nabssum_F7                37.050789\nssi_F7                  820.393141\nvar_F7.1                 64.600919\nmav_F7                    2.570039\nlog_F7                    1.583103\naac_F7                    1.645498\ndasdv_F7                  2.552779\ndfa_F7                    1.858145\nfuzzy_F7               5446.766319\nshannon_F7                0.253003\npe_F7                     0.204220\nhjorth_F7                 0.155269\nhurst_F7                  0.286486\nlabel                     0.817206\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":["data_source.skew()"],"metadata":{"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"subject no.             0.000000\nmean_F7                 2.874945\nmax_F7                  2.339593\nmin_F7                 12.535919\nstd_F7                  2.372759\niqr_F7                  3.089990\nvar_F7                  4.045615\nsum_F7                  2.725090\nskw_F7                  0.029088\nkrt_F7                  0.939747\nmean_first_diff_F7      2.526946\nmean_second_diff_F7     2.882766\nrms_F7                  2.415462\nabssum_F7               2.725090\nssi_F7                  4.024341\nvar_F7.1                4.459129\nmav_F7                  2.874945\nlog_F7                  5.514621\naac_F7                  2.492809\ndasdv_F7                2.287237\ndfa_F7                  3.073314\nfuzzy_F7               11.905649\nshannon_F7             -0.034860\npe_F7                  -0.640917\nhjorth_F7              -0.033445\nhurst_F7                0.668152\nlabel                   0.000000\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# random split the dataset (do this among each people)\n","def split_among_people(data_source):\n","    people_num = len(data_source['subject no.'].unique())\n","    train_df_list = []\n","    test_df_list = []\n","    for idx in range(people_num):\n","        train_df, test_df = train_test_split(data_source[data_source['subject no.'] == (idx + 1)], test_size=0.3, random_state=7)\n","        train_df_list.append(train_df)\n","        test_df_list.append(test_df)\n","    return pd.concat(train_df_list), pd.concat(test_df_list)\n","\n","def model_report(model, tar_x, tar_y):\n","    pred = model.predict(tar_x)\n","    acc = accuracy_score(tar_y, pred)\n","    print(\"accuracy: \", acc)\n","    cm = confusion_matrix(tar_y, pred)\n","    print(\"confusion matrix:\\n\",cm)"],"metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = split_among_people(data_source)\n","train_y = train_data.label\n","train_x = train_data.drop(columns=['label', 'subject no.'])\n","test_y = test_data.label\n","test_x = test_data.drop(columns=['label', 'subject no.'])\n","\n","# RandomForest baseline\n","randomForest = RandomForestClassifier(random_state=7)\n","randomForest.fit(train_x, train_y)\n","\n","print(\"RF on training set:\")\n","model_report(randomForest, train_x, train_y)\n","print(\"\\nRF on testing set:\")\n","model_report(randomForest, test_x, test_y)"],"metadata":{"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"RF on training set:\naccuracy:  1.0\nconfusion matrix:\n [[112   0   0]\n [  0 128   0]\n [  0   0 144]]\n\nRF on testing set:\naccuracy:  0.4322916666666667\nconfusion matrix:\n [[27 25 28]\n [15 29 20]\n [11 10 27]]\n","output_type":"stream"}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","feature_num = len(train_x.columns)\n","latent_var_dim = 32\n","class_num = len(train_y.unique())\n","batch_size = 4\n","epoch_num = 800\n","lr = .002\n","\n","# a simple nn model\n","class BasicNNClassifier(torch.nn.Module):\n","    def __init__(self, feature_num, latent_var_dim, class_num, dropout=.1):\n","        super(BasicNNClassifier, self).__init__()\n","        self.input_dim = feature_num\n","        self.hidden_dim = latent_var_dim\n","        self.output_dim = class_num\n","        self.layers = torch.nn.Sequential(\n","            torch.nn.Linear(self.input_dim, self.hidden_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(self.hidden_dim, self.hidden_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(self.hidden_dim, self.output_dim),\n","        )\n","    def forward(self, x):\n","        output = self.layers(x)\n","        return output"],"metadata":{"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# Now: 0=classical, 1=instrumental, 2=pop\n","# train_yt = torch.Tensor(pd.get_dummies(train_y).values) - 1\n","train_yt = torch.LongTensor(train_y.values) - 1\n","train_xt = torch.Tensor(train_x.values)\n","test_yt = torch.LongTensor(test_y.values) - 1\n","test_xt = torch.Tensor(test_x.values)\n","\n","train_dataset = TensorDataset(train_xt, train_yt)\n","train_sampler = RandomSampler(train_dataset)\n","train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n","\n","basic_model = BasicNNClassifier(feature_num, latent_var_dim, class_num).to(device)\n","\n","optimiser = torch.optim.Adam(basic_model.parameters(), lr=lr)\n","loss_func = torch.nn.CrossEntropyLoss()\n","\n","basic_model.train()"],"metadata":{"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"BasicNNClassifier(\n  (layers): Sequential(\n    (0): Linear(in_features=25, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=32, out_features=32, bias=True)\n    (3): ReLU()\n    (4): Dropout(p=0.1, inplace=False)\n    (5): Linear(in_features=32, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":["for epoch in range(epoch_num):\n","    epoch_loss = 0\n","    for batch, (data, label) in enumerate(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        pred = basic_model(data)\n","        loss = loss_func(pred.squeeze(), label.squeeze())\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","    if (epoch + 1) % 100 == 0:\n","        train_pred = torch.argmax(basic_model(train_xt.to(device)), dim=1)\n","        acc = accuracy_score(train_pred.cpu().squeeze().detach().numpy(), train_yt.squeeze().detach().numpy())\n","        print(\"Now epoch:\", epoch + 1, \"current loss:\", epoch_loss, \"train set accuracy:\", acc)"],"metadata":{"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Now epoch: 100 current loss: 90.02490285038948 train set accuracy: 0.5\nNow epoch: 200 current loss: 77.03446833789349 train set accuracy: 0.6171875\nNow epoch: 300 current loss: 68.9581187069416 train set accuracy: 0.65625\nNow epoch: 400 current loss: 64.35763330757618 train set accuracy: 0.7083333333333334\nNow epoch: 500 current loss: 59.51258658617735 train set accuracy: 0.75\nNow epoch: 600 current loss: 68.50920137390494 train set accuracy: 0.7473958333333334\nNow epoch: 700 current loss: 57.922345355153084 train set accuracy: 0.7473958333333334\nNow epoch: 800 current loss: 45.8806713463855 train set accuracy: 0.7890625\n","output_type":"stream"}]},{"cell_type":"code","source":["basic_model.eval()\n","test_pred = torch.argmax(basic_model(test_xt.to(device)), dim=1)\n","fin_acc = accuracy_score(test_pred.cpu().squeeze().detach().numpy(), test_yt.squeeze().detach().numpy())\n","fin_acc"],"metadata":{"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"0.3072916666666667"},"metadata":{}}]},{"cell_type":"code","source":["latent_var_dim = 32\n","batch_size = 4\n","epoch_num = 800\n","lr = .002\n","\n","class DeeperNNClassifier(torch.nn.Module):\n","    def __init__(self, feature_num, latent_var_dim, class_num, dropout=.1):\n","        super(DeeperNNClassifier, self).__init__()\n","        self.input_dim = feature_num\n","        self.hidden_dim = latent_var_dim\n","        self.output_dim = class_num\n","        self.layers = torch.nn.Sequential(\n","            torch.nn.Linear(self.input_dim, self.hidden_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(self.hidden_dim, self.hidden_dim//2),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(self.hidden_dim//2, self.hidden_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(self.hidden_dim, self.output_dim)\n","        )\n","    def forward(self, x):\n","        output = self.layers(x)\n","        return output"],"metadata":{"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["deeper_model = DeeperNNClassifier(feature_num, latent_var_dim, class_num).to(device)\n","optimiser = torch.optim.Adam(deeper_model.parameters(), lr=lr)\n","deeper_model.train()\n","\n","for epoch in range(epoch_num):\n","    epoch_loss = 0\n","    for batch, (data, label) in enumerate(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        pred = deeper_model(data)\n","        loss = loss_func(pred.squeeze(), label.squeeze())\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","    if (epoch + 1) % 100 == 0:\n","        train_pred = torch.argmax(deeper_model(train_xt.to(device)), dim=1)\n","        acc = accuracy_score(train_pred.cpu().squeeze().detach().numpy(), train_yt.squeeze().detach().numpy())\n","        print(\"Now epoch:\", epoch + 1, \"current loss:\", epoch_loss, \"train set accuracy:\", acc)"],"metadata":{"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Now epoch: 100 current loss: 99.01782315969467 train set accuracy: 0.4427083333333333\nNow epoch: 200 current loss: 89.42713230848312 train set accuracy: 0.5572916666666666\nNow epoch: 300 current loss: 82.69496890902519 train set accuracy: 0.6041666666666666\nNow epoch: 400 current loss: 76.9210142493248 train set accuracy: 0.6171875\nNow epoch: 500 current loss: 77.57702323794365 train set accuracy: 0.640625\nNow epoch: 600 current loss: 70.49718227237463 train set accuracy: 0.6223958333333334\nNow epoch: 700 current loss: 70.31545978784561 train set accuracy: 0.6692708333333334\nNow epoch: 800 current loss: 72.68914490938187 train set accuracy: 0.6510416666666666\n","output_type":"stream"}]},{"cell_type":"code","source":["deeper_model.eval()\n","test_pred = torch.argmax(deeper_model(test_xt.to(device)), dim=1)\n","fin_acc = accuracy_score(test_pred.cpu().squeeze().detach().numpy(), test_yt.squeeze().detach().numpy())\n","fin_acc"],"metadata":{"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"0.3385416666666667"},"metadata":{}}]},{"cell_type":"code","source":["# deal with some ridiculous outliers (only in train samples)\n","def norm_remove_outliers(train_df, show_ratio=False):\n","    norm_train_x = train_df.copy()\n","    normalise_dict = {}\n","    train_features = norm_train_x.columns\n","    normalise_dict['feature_list'] = train_features\n","    for col in train_features:\n","        ratio = (norm_train_x[col].max()-norm_train_x[col].quantile(.97))/(norm_train_x[col].max()-norm_train_x[col].min())\n","        flag = ratio > .25\n","        if flag:\n","            # deal with outliers\n","            IQR = norm_train_x[col].quantile(.75) - norm_train_x[col].quantile(.25)\n","            upper = min(norm_train_x[col].quantile(.75) + 1.5 * IQR, norm_train_x[col].max())\n","            lower = max(norm_train_x[col].quantile(.25) - 1.5 * IQR, norm_train_x[col].min())\n","            norm_train_x[col] = (norm_train_x[col] - lower) / (upper - lower)\n","            norm_train_x[col][norm_train_x[col] < 0] = 0\n","            norm_train_x[col][norm_train_x[col] > 1] = 1\n","        else:\n","            # do the simple normalise\n","            upper = norm_train_x[col].max()\n","            lower = norm_train_x[col].min()\n","            norm_train_x[col] = (norm_train_x[col] - lower) / (upper - lower)\n","        # keep the parameters we use\n","        normalise_dict[col + \"_upper\"] = upper\n","        normalise_dict[col + \"_lower\"] = lower\n","        if show_ratio:\n","            print(col, ratio, flag)\n","    return norm_train_x, normalise_dict\n","\n","norm_train_x, normalise_dict = norm_remove_outliers(train_x, True)"],"metadata":{"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"mean_F7 0.587600573684096 True\nmax_F7 0.33221822575045706 True\nmin_F7 0.9017276051911405 True\nstd_F7 0.38954179450915727 True\niqr_F7 0.5198141333913147 True\nvar_F7 0.6255487399789161 True\nsum_F7 0.44931092831542574 True\nskw_F7 0.046924959858395794 False\nkrt_F7 0.08935693588640761 False\nmean_first_diff_F7 0.4394863643797536 True\nmean_second_diff_F7 0.4977530773093126 True\nrms_F7 0.39685216505161736 True\nabssum_F7 0.44931092831542574 True\nssi_F7 0.6405419443622601 True\nvar_F7.1 0.6492384148914431 True\nmav_F7 0.587600573684096 True\nlog_F7 0.7316227007878546 True\naac_F7 0.478676247446412 True\ndasdv_F7 0.2969239149643623 True\ndfa_F7 0.6204093261879456 True\nfuzzy_F7 0.999664116407571 True\nshannon_F7 0.033714249946342686 False\npe_F7 0.03612359947967791 False\nhjorth_F7 0.07963521952264878 False\nhurst_F7 0.15327962842146023 False\n","output_type":"stream"}]},{"cell_type":"code","source":["norm_train_x.describe()"],"metadata":{"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"          mean_F7      max_F7      min_F7      std_F7      iqr_F7      var_F7  \\\ncount  384.000000  384.000000  384.000000  384.000000  384.000000  384.000000   \nmean     0.336504    0.306563    0.238296    0.301913    0.403639    0.261670   \nstd      0.325871    0.349433    0.336286    0.342396    0.308063    0.389950   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.100966    0.070183    0.009026    0.073781    0.181933    0.014703   \n50%      0.207922    0.129223    0.024338    0.127511    0.287229    0.038740   \n75%      0.460580    0.442110    0.405415    0.444269    0.509160    0.408822   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n           sum_F7      skw_F7      krt_F7  mean_first_diff_F7  ...  \\\ncount  384.000000  384.000000  384.000000          384.000000  ...   \nmean     0.315287    0.589774    0.323357            0.312938  ...   \nstd      0.324564    0.209062    0.267521            0.336076  ...   \nmin      0.000000    0.000000    0.000000            0.000000  ...   \n25%      0.089072    0.446245    0.107531            0.077859  ...   \n50%      0.166081    0.565754    0.229947            0.153518  ...   \n75%      0.453443    0.741710    0.486059            0.446715  ...   \nmax      1.000000    1.000000    1.000000            1.000000  ...   \n\n           mav_F7      log_F7      aac_F7    dasdv_F7      dfa_F7    fuzzy_F7  \\\ncount  384.000000  384.000000  384.000000  384.000000  384.000000  384.000000   \nmean     0.336504    0.316817    0.308560    0.311396    0.315896    0.367647   \nstd      0.325871    0.317529    0.338069    0.354837    0.347904    0.330414   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.100966    0.069296    0.070031    0.067679    0.065025    0.128934   \n50%      0.207922    0.183206    0.146878    0.131495    0.141669    0.218570   \n75%      0.460580    0.441578    0.442019    0.440608    0.439015    0.477360   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n       shannon_F7       pe_F7   hjorth_F7    hurst_F7  \ncount  384.000000  384.000000  384.000000  384.000000  \nmean     0.614787    0.630686    0.484858    0.382746  \nstd      0.227990    0.290147    0.197035    0.212048  \nmin      0.000000    0.000000    0.000000    0.000000  \n25%      0.400080    0.301030    0.353754    0.238620  \n50%      0.630309    0.790755    0.522187    0.316119  \n75%      0.756550    0.877995    0.603199    0.518824  \nmax      1.000000    1.000000    1.000000    1.000000  \n\n[8 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_F7</th>\n      <th>max_F7</th>\n      <th>min_F7</th>\n      <th>std_F7</th>\n      <th>iqr_F7</th>\n      <th>var_F7</th>\n      <th>sum_F7</th>\n      <th>skw_F7</th>\n      <th>krt_F7</th>\n      <th>mean_first_diff_F7</th>\n      <th>...</th>\n      <th>mav_F7</th>\n      <th>log_F7</th>\n      <th>aac_F7</th>\n      <th>dasdv_F7</th>\n      <th>dfa_F7</th>\n      <th>fuzzy_F7</th>\n      <th>shannon_F7</th>\n      <th>pe_F7</th>\n      <th>hjorth_F7</th>\n      <th>hurst_F7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>...</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n      <td>384.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.336504</td>\n      <td>0.306563</td>\n      <td>0.238296</td>\n      <td>0.301913</td>\n      <td>0.403639</td>\n      <td>0.261670</td>\n      <td>0.315287</td>\n      <td>0.589774</td>\n      <td>0.323357</td>\n      <td>0.312938</td>\n      <td>...</td>\n      <td>0.336504</td>\n      <td>0.316817</td>\n      <td>0.308560</td>\n      <td>0.311396</td>\n      <td>0.315896</td>\n      <td>0.367647</td>\n      <td>0.614787</td>\n      <td>0.630686</td>\n      <td>0.484858</td>\n      <td>0.382746</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.325871</td>\n      <td>0.349433</td>\n      <td>0.336286</td>\n      <td>0.342396</td>\n      <td>0.308063</td>\n      <td>0.389950</td>\n      <td>0.324564</td>\n      <td>0.209062</td>\n      <td>0.267521</td>\n      <td>0.336076</td>\n      <td>...</td>\n      <td>0.325871</td>\n      <td>0.317529</td>\n      <td>0.338069</td>\n      <td>0.354837</td>\n      <td>0.347904</td>\n      <td>0.330414</td>\n      <td>0.227990</td>\n      <td>0.290147</td>\n      <td>0.197035</td>\n      <td>0.212048</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.100966</td>\n      <td>0.070183</td>\n      <td>0.009026</td>\n      <td>0.073781</td>\n      <td>0.181933</td>\n      <td>0.014703</td>\n      <td>0.089072</td>\n      <td>0.446245</td>\n      <td>0.107531</td>\n      <td>0.077859</td>\n      <td>...</td>\n      <td>0.100966</td>\n      <td>0.069296</td>\n      <td>0.070031</td>\n      <td>0.067679</td>\n      <td>0.065025</td>\n      <td>0.128934</td>\n      <td>0.400080</td>\n      <td>0.301030</td>\n      <td>0.353754</td>\n      <td>0.238620</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.207922</td>\n      <td>0.129223</td>\n      <td>0.024338</td>\n      <td>0.127511</td>\n      <td>0.287229</td>\n      <td>0.038740</td>\n      <td>0.166081</td>\n      <td>0.565754</td>\n      <td>0.229947</td>\n      <td>0.153518</td>\n      <td>...</td>\n      <td>0.207922</td>\n      <td>0.183206</td>\n      <td>0.146878</td>\n      <td>0.131495</td>\n      <td>0.141669</td>\n      <td>0.218570</td>\n      <td>0.630309</td>\n      <td>0.790755</td>\n      <td>0.522187</td>\n      <td>0.316119</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.460580</td>\n      <td>0.442110</td>\n      <td>0.405415</td>\n      <td>0.444269</td>\n      <td>0.509160</td>\n      <td>0.408822</td>\n      <td>0.453443</td>\n      <td>0.741710</td>\n      <td>0.486059</td>\n      <td>0.446715</td>\n      <td>...</td>\n      <td>0.460580</td>\n      <td>0.441578</td>\n      <td>0.442019</td>\n      <td>0.440608</td>\n      <td>0.439015</td>\n      <td>0.477360</td>\n      <td>0.756550</td>\n      <td>0.877995</td>\n      <td>0.603199</td>\n      <td>0.518824</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["# keep the current df to take a look \n","current_df = norm_train_x.copy()\n","current_df['label'] = train_y\n","current_df.to_csv('normed_train_df.csv')"],"metadata":{"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# define the normalise pipeline for the test set\n","def norm_test_df(test_df):\n","    norm_test_df = test_df.copy()\n","    for col in normalise_dict['feature_list']:\n","        if col in norm_test_df.columns:\n","            norm_test_df[col] = (norm_test_df[col] - normalise_dict[col + \"_lower\"]) / (normalise_dict[col + \"_upper\"] - normalise_dict[col + \"_lower\"])\n","            norm_test_df[col][norm_test_df[col] < 0] = 0\n","            norm_test_df[col][norm_test_df[col] > 1] = 1\n","    return norm_test_df\n","\n","# this one for the input samples (as vectors)\n","def norm_input_vec(sample):\n","    idx = 0\n","    for col in normalise_dict['feature_list']:\n","        sample[idx] = min(1, max(0, (sample[idx] - normalise_dict[col + \"_lower\"])) / (normalise_dict[col + \"_upper\"] - normalise_dict[col + \"_lower\"]))\n","        idx += 1\n","    return sample"],"metadata":{"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["feature_num = len(norm_train_x.columns)\n","latent_var_dim = 32\n","class_num = len(train_y.unique())\n","batch_size = 8\n","epoch_num = 1600\n","lr = .001\n","\n","# Now: 0=classical, 1=instrumental, 2=pop\n","train_yt = torch.LongTensor(train_y.values) - 1\n","train_xt = torch.Tensor(norm_train_x.values)\n","norm_test_x = norm_test_df(test_x)\n","test_yt = torch.LongTensor(test_y.values) - 1\n","test_xt = torch.Tensor(norm_test_x.values)\n","\n","train_dataset = TensorDataset(train_xt, train_yt)\n","train_sampler = RandomSampler(train_dataset)\n","train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n","\n","# Better performance after removing outliers and normalisation?\n","deeper_model = DeeperNNClassifier(feature_num, latent_var_dim, class_num).to(device)\n","optimiser = torch.optim.Adam(deeper_model.parameters(), lr=lr)\n","loss_func = torch.nn.CrossEntropyLoss()\n","\n","deeper_model.train()\n","for epoch in range(epoch_num):\n","    epoch_loss = 0\n","    for batch, (data, label) in enumerate(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        pred = deeper_model(data)\n","        loss = loss_func(pred.squeeze(), label.squeeze())\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","    if (epoch + 1) % 100 == 0:\n","        train_pred = torch.argmax(deeper_model(train_xt.to(device)), dim=1)\n","        acc = accuracy_score(train_pred.cpu().squeeze().detach().numpy(), train_yt.squeeze().detach().numpy())\n","        print(\"Now epoch:\", epoch + 1, \"current loss:\", epoch_loss, \"train set accuracy:\", acc)"],"metadata":{"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Now epoch: 100 current loss: 45.0955405831337 train set accuracy: 0.5364583333333334\nNow epoch: 200 current loss: 39.12892943620682 train set accuracy: 0.6276041666666666\nNow epoch: 300 current loss: 32.73028127849102 train set accuracy: 0.7005208333333334\nNow epoch: 400 current loss: 28.092902541160583 train set accuracy: 0.734375\nNow epoch: 500 current loss: 27.423722073435783 train set accuracy: 0.7890625\nNow epoch: 600 current loss: 22.13586027920246 train set accuracy: 0.8072916666666666\nNow epoch: 700 current loss: 20.060323998332024 train set accuracy: 0.8463541666666666\nNow epoch: 800 current loss: 22.35003037750721 train set accuracy: 0.8359375\nNow epoch: 900 current loss: 19.260108843445778 train set accuracy: 0.8411458333333334\nNow epoch: 1000 current loss: 19.143173776566982 train set accuracy: 0.8619791666666666\nNow epoch: 1100 current loss: 15.252820432186127 train set accuracy: 0.8671875\nNow epoch: 1200 current loss: 15.46901073306799 train set accuracy: 0.875\nNow epoch: 1300 current loss: 16.763387862592936 train set accuracy: 0.8671875\nNow epoch: 1400 current loss: 16.58218475431204 train set accuracy: 0.8515625\nNow epoch: 1500 current loss: 16.51477825269103 train set accuracy: 0.8541666666666666\nNow epoch: 1600 current loss: 13.926194876432419 train set accuracy: 0.875\n","output_type":"stream"}]},{"cell_type":"code","source":["normed_test_df = norm_test_x.copy()\n","normed_test_df['label'] = test_y\n","normed_test_df.to_csv('normed_test_df.csv')"],"metadata":{"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["deeper_model.eval()\n","test_pred = torch.argmax(deeper_model(test_xt.to(device)), dim=1)\n","fin_acc = accuracy_score(test_pred.cpu().squeeze().detach().numpy(), test_yt.squeeze().detach().numpy())\n","fin_acc"],"metadata":{"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"0.3177083333333333"},"metadata":{}}]},{"cell_type":"code","source":["# Check the correlation, we may not need all the features (Pearson)\n","corr_map = current_df.corr()\n","corr_map.label"],"metadata":{"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"mean_F7               -0.022965\nmax_F7                 0.044260\nmin_F7                -0.091819\nstd_F7                 0.024032\niqr_F7                -0.072700\nvar_F7                 0.034275\nsum_F7                 0.041123\nskw_F7                 0.152965\nkrt_F7                 0.218442\nmean_first_diff_F7     0.012289\nmean_second_diff_F7    0.015791\nrms_F7                 0.009604\nabssum_F7              0.041123\nssi_F7                 0.049943\nvar_F7.1               0.014550\nmav_F7                -0.022965\nlog_F7                -0.075713\naac_F7                 0.016108\ndasdv_F7               0.044517\ndfa_F7                -0.017972\nfuzzy_F7              -0.042331\nshannon_F7            -0.119292\npe_F7                  0.086099\nhjorth_F7              0.034260\nhurst_F7               0.021555\nlabel                  1.000000\nName: label, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":["high_corr_features = []\n","for col in corr_map:\n","    if abs(corr_map[col].label) > .02 and col != 'label':\n","        high_corr_features.append(col)\n","# the training features' distributions according to different labels\n","diff_dis_features = ['max_F7', 'min_F7', 'std_F7', 'iqr_F7', 'var_F7', 'sum_F7', 'skw_F7', 'krt_F7', 'mean_second_diff_F7',\n","                     'abssum_F7', 'ssi_F7', 'log_F7', 'dasdv_F7', 'aac_F7', 'shannon_F7', 'hjorth_F7', 'hurst_F7']"],"metadata":{"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["selected_features = []\n","for col in diff_dis_features:\n","    if col in high_corr_features:\n","        selected_features.append(col)"],"metadata":{"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["selected_features"],"metadata":{"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"['max_F7',\n 'min_F7',\n 'std_F7',\n 'iqr_F7',\n 'var_F7',\n 'sum_F7',\n 'skw_F7',\n 'krt_F7',\n 'abssum_F7',\n 'ssi_F7',\n 'log_F7',\n 'dasdv_F7',\n 'shannon_F7',\n 'hjorth_F7',\n 'hurst_F7']"},"metadata":{}}]},{"cell_type":"code","source":["# refine the nn model (so we can check the middle outputs)\n","class RefinedNNClassifier(torch.nn.Module):\n","    \n","    def __init__(self, feature_num, latent_var_dim, class_num, dropout=.1):\n","        super(RefinedNNClassifier, self).__init__()\n","        self.input_dim = feature_num\n","        self.hidden_dim = latent_var_dim\n","        self.output_dim = class_num\n","        self.eval_flag = False\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.activation = torch.nn.ReLU()\n","        self.input_layer = torch.nn.Linear(feature_num, latent_var_dim)\n","        self.squeeze_layer = torch.nn.Sequential(\n","            torch.nn.Linear(latent_var_dim, latent_var_dim//2),\n","            torch.nn.Linear(latent_var_dim//2, latent_var_dim)\n","        )\n","        self.output_layer = torch.nn.Linear(latent_var_dim, class_num)\n","        \n","    def turning_eval_flag(self, flag):\n","        self.eval_flag = flag\n","        \n","    def forward(self, x, check_flag=0):\n","        check_vec = None\n","        # the input layer\n","        output = self.input_layer(x)\n","        output = self.activation(output)\n","        if check_flag == 1:\n","            check_vec = output\n","        output = self.dropout(output)\n","        # the middle suqeeze layer\n","        output = self.squeeze_layer(output)\n","        output = self.activation(output)\n","        if check_flag == 2:\n","            check_vec = output\n","        # the output layer\n","        output = self.output_layer(output)\n","        if self.eval_flag:\n","            output = torch.nn.functional.softmax(output, dim=1)\n","        if check_flag != 0:\n","            return output, check_vec\n","        else:\n","            return output"],"metadata":{"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["# now check if the data is still that noisy\n","feature_num = len(selected_features)\n","latent_var_dim = 32\n","class_num = len(train_y.unique())\n","batch_size = 8\n","epoch_num = 1600\n","lr = .001\n","\n","selected_train_x = norm_train_x[selected_features]\n","selected_test_x = norm_test_df(test_x)[selected_features]\n","\n","# Now: 0=classical, 1=instrumental, 2=pop\n","train_yt = torch.LongTensor(train_y.values) - 1\n","train_xt = torch.Tensor(selected_train_x.values)\n","test_yt = torch.LongTensor(test_y.values) - 1\n","test_xt = torch.Tensor(selected_test_x.values)\n","\n","train_dataset = TensorDataset(train_xt, train_yt)\n","train_sampler = RandomSampler(train_dataset)\n","train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n","\n","# Better performance after removing outliers and normalisation?\n","refine_model = RefinedNNClassifier(feature_num, latent_var_dim, class_num).to(device)\n","optimiser = torch.optim.Adam(refine_model.parameters(), lr=lr)\n","loss_func = torch.nn.CrossEntropyLoss()\n","\n","refine_model.train()\n","refine_model.turning_eval_flag(False)\n","for epoch in range(epoch_num):\n","    epoch_loss = 0\n","    for batch, (data, label) in enumerate(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        pred = refine_model(data)\n","        loss = loss_func(pred.squeeze(), label.squeeze())\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","    if (epoch + 1) % 100 == 0:\n","        train_pred = torch.argmax(refine_model(train_xt.to(device)), dim=1)\n","        acc = accuracy_score(train_pred.cpu().squeeze().detach().numpy(), train_yt.squeeze().detach().numpy())\n","        print(\"Now epoch:\", epoch + 1, \"current loss:\", epoch_loss, \"train set accuracy:\", acc)"],"metadata":{"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Now epoch: 100 current loss: 45.802460968494415 train set accuracy: 0.4921875\nNow epoch: 200 current loss: 42.08914226293564 train set accuracy: 0.5625\nNow epoch: 300 current loss: 39.915780663490295 train set accuracy: 0.5989583333333334\nNow epoch: 400 current loss: 35.98348733782768 train set accuracy: 0.6432291666666666\nNow epoch: 500 current loss: 33.00721272826195 train set accuracy: 0.6953125\nNow epoch: 600 current loss: 31.57929304242134 train set accuracy: 0.7317708333333334\nNow epoch: 700 current loss: 29.230279475450516 train set accuracy: 0.7421875\nNow epoch: 800 current loss: 25.542699232697487 train set accuracy: 0.7734375\nNow epoch: 900 current loss: 25.25442785024643 train set accuracy: 0.8203125\nNow epoch: 1000 current loss: 23.215742006897926 train set accuracy: 0.7916666666666666\nNow epoch: 1100 current loss: 20.301844641566277 train set accuracy: 0.8541666666666666\nNow epoch: 1200 current loss: 21.03086981922388 train set accuracy: 0.8359375\nNow epoch: 1300 current loss: 17.608580470085144 train set accuracy: 0.8645833333333334\nNow epoch: 1400 current loss: 18.92641858011484 train set accuracy: 0.8567708333333334\nNow epoch: 1500 current loss: 20.914595812559128 train set accuracy: 0.8619791666666666\nNow epoch: 1600 current loss: 21.513387352228165 train set accuracy: 0.8541666666666666\n","output_type":"stream"}]},{"cell_type":"code","source":["refine_model.eval()\n","refine_model.turning_eval_flag(True)\n","test_pred = torch.argmax(refine_model(test_xt.to(device)), dim=1)\n","fin_acc = accuracy_score(test_pred.cpu().squeeze().detach().numpy(), test_yt.squeeze().detach().numpy())\n","fin_acc"],"metadata":{"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"0.3645833333333333"},"metadata":{}}]},{"cell_type":"code","source":["# PROGRESSIVE IMAGE COMPRESSION\n","test_patterns = train_xt[:10]\n","refine_model.to('cpu')\n","pred, hidden_output = refine_model(test_patterns, check_flag=1)\n","hidden_output = hidden_output.T\n","hidden_output.size()"],"metadata":{"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 10])"},"metadata":{}}]},{"cell_type":"code","source":["def get_angle_dict(hidden_output):\n","    angle_dict = {}\n","    for idx_1 in range(hidden_output.size()[0]):\n","        for idx_2 in range(idx_1+1, hidden_output.size()[0]):\n","            dot_prod = hidden_output[idx_1].T @ hidden_output[idx_2]\n","            norm_len = (hidden_output[idx_1].T @ hidden_output[idx_1]).sqrt() * (hidden_output[idx_2].T @ hidden_output[idx_2]).sqrt()\n","            if norm_len == 0:\n","                continue\n","            angle_dict[str(idx_1+1)+str(idx_2+1)] = (torch.arccos(dot_prod / norm_len) / np.pi) * 180\n","    return angle_dict\n","angle_dict = get_angle_dict(hidden_output)"],"metadata":{"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["def get_min_suit_angle(angle_dict, report_flag=False):\n","    min_angle = 180\n","    num_good_angles = len(angle_dict)\n","    for item in angle_dict:\n","        if angle_dict[item] < min_angle:\n","            min_angle = angle_dict[item]\n","        if angle_dict[item] < 15 or angle_dict[item] > 165:\n","            num_good_angles -=1\n","    if report_flag:\n","        print(\"The minium angle:\", min_angle, \"\\nNumber of good angles:\", num_good_angles)\n","    suitable_units_num = int(np.sqrt(num_good_angles * 2 + .25) - .5) + 1\n","    return min_angle, suitable_units_num\n","min_angle, num_suit_angles = get_min_suit_angle(angle_dict, True)"],"metadata":{"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"The minium angle: tensor(0., grad_fn=<MulBackward0>) \nNumber of good angles: 226\n","output_type":"stream"}]},{"cell_type":"code","source":["num_suit_angles"],"metadata":{"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"21"},"metadata":{}}]},{"cell_type":"code","source":["# Now train the models and draw the plot according to the minium angle vs. final loss / latent variable nums\n","lartent_var_num_list = []\n","min_angle_list = []\n","fin_loss_list = []\n","\n","feature_num = len(selected_features)\n","class_num = len(train_y.unique())\n","batch_size = 8\n","epoch_num = 1600\n","lr = .001\n","\n","selected_train_x = norm_train_x[selected_features]\n","selected_test_x = norm_test_df(test_x)[selected_features]\n","\n","# Now: 0=classical, 1=instrumental, 2=pop\n","train_yt = torch.LongTensor(train_y.values) - 1\n","train_xt = torch.Tensor(selected_train_x.values)\n","test_yt = torch.LongTensor(test_y.values) - 1\n","test_xt = torch.Tensor(selected_test_x.values)\n","\n","train_dataset = TensorDataset(train_xt, train_yt)\n","train_sampler = RandomSampler(train_dataset)\n","train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n","loss_func = torch.nn.CrossEntropyLoss()\n","\n","for latent_var_dim in range(4, 34, 2):\n","    refine_model = RefinedNNClassifier(feature_num, latent_var_dim, class_num).to(device)\n","    optimiser = torch.optim.Adam(refine_model.parameters(), lr=lr)\n","    refine_model.train()\n","    refine_model.turning_eval_flag(False)\n","    for epoch in range(epoch_num):\n","        for batch, (data, label) in enumerate(train_loader):\n","            data = data.to(device)\n","            label = label.to(device)\n","            pred = refine_model(data)\n","            loss = loss_func(pred.squeeze(), label.squeeze())\n","            optimiser.zero_grad()\n","            loss.backward()\n","            optimiser.step()\n","    fin_pred, fin_hidden = refine_model(train_xt.to(device), check_flag=1)\n","    fin_loss = loss_func(fin_pred.squeeze(), train_yt.squeeze()).cpu().data.numpy()\n","    fin_angle_dict = get_angle_dict(fin_hidden.T)\n","    fin_min_angle, _ = get_min_suit_angle(fin_angle_dict)\n","    print(\"Now with lartent var num:\", latent_var_dim, \"\\nCurrent min angle is:\", fin_min_angle, \"\\nThe loss on the training set:\", fin_loss)\n","    lartent_var_num_list.append(latent_var_dim)\n","    min_angle_list.append(fin_min_angle)\n","    fin_loss_list.append(fin_loss)"],"metadata":{"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Now with lartent var num: 4 \nCurrent min angle is: tensor(78.6367, grad_fn=<MulBackward0>) \nThe loss on the training set: 1.0049015\nNow with lartent var num: 6 \nCurrent min angle is: tensor(40.7090, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.9930292\nNow with lartent var num: 8 \nCurrent min angle is: tensor(62.5405, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.88018376\nNow with lartent var num: 10 \nCurrent min angle is: tensor(34.1788, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.8642518\nNow with lartent var num: 12 \nCurrent min angle is: tensor(42.5126, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.73458225\nNow with lartent var num: 14 \nCurrent min angle is: tensor(34.0916, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.60326934\nNow with lartent var num: 16 \nCurrent min angle is: tensor(21.9105, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.6830488\nNow with lartent var num: 18 \nCurrent min angle is: tensor(37.5679, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.6550665\nNow with lartent var num: 20 \nCurrent min angle is: tensor(18.0624, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.63350755\nNow with lartent var num: 22 \nCurrent min angle is: tensor(9.0379, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.65109235\nNow with lartent var num: 24 \nCurrent min angle is: tensor(29.2991, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.5952282\nNow with lartent var num: 26 \nCurrent min angle is: tensor(16.1430, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.54868215\nNow with lartent var num: 28 \nCurrent min angle is: tensor(19.1899, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.4304513\nNow with lartent var num: 30 \nCurrent min angle is: tensor(27.5728, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.43337014\nNow with lartent var num: 32 \nCurrent min angle is: tensor(21.0948, grad_fn=<MulBackward0>) \nThe loss on the training set: 0.37628603\n","output_type":"stream"}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt \n","\n","plt.plot(min_angle_list,lartent_var_num_list)\n","plt.show()\n","plt.plot(fin_loss_list,lartent_var_num_list)\n","plt.show()"],"metadata":{"trusted":true},"execution_count":87,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}